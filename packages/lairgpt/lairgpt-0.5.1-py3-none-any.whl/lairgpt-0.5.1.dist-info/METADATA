Metadata-Version: 2.1
Name: lairgpt
Version: 0.5.1
Summary: A Pytorch-based package by LightOn AI Research allowing to perform inference with PAGnol models.
Home-page: https://github.com/lightonai/lairgpt
Author: Lighton AI Research
Author-email: iacopo@lighton.io,julien@lighton.io,igor@lighton.io
License: UNKNOWN
Platform: UNKNOWN
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Description-Content-Type: text/markdown
Requires-Dist: torch (==1.8.*)
Requires-Dist: tokenizers (==0.10)
Requires-Dist: wget (==3.2)

# <img src="https://cloud.lighton.ai/wp-content/uploads/2020/01/LightOnCloud.png" width=50/>LairGPT

[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)  [![Twitter](https://img.shields.io/twitter/follow/LightOnIO?style=social)](https://twitter.com/LightOnIO)

A Python package in Pytorch by [LightOn AI Research](https://lair.lighton.ai/) that allows to perform inference
with [PAGnol models](https://lair.lighton.ai/pagnol/).
You can test the generation capabilities of PAGnol on our [interactive demo website](https://pagnol.lighton.ai/).

## Install

### Requirements

The package is tested with Python 3.9. After cloning this repository, you can create a `conda` environment
with the necessary dependencies from its root by

```
conda env create --file=environment.yml
```

If you prefer control on your environment, the dependencies are

```
pytorch==1.8.1
tokenizers==0.10
python-wget==3.2
```

### pip

Simply run `pip install .` from the root of this repository.

## Text generation

The simplest way to generate text with PAGnol using `lairgpt` is

```
from lairgpt.models import PAGnol

pagnol = PAGnol.small()
pagnol("Salut PAGnol, comment Ã§a va ?")
```

We include a demo script `main.py` in this repository that takes the path to models and tokenizers, and an input text, and generates sentences from it.
To use it:

```
python main.py --size large --text "LightOn est une startup technologique"
```

To generate text we rely on the `infer` method of the `TextGenerator` class that takes the usual parameters:
- `mode`: (default: `"nucleus"`)
  - `"greedy"`: always select the most likely word as its next word.
  - `"top-k"`:  filter to the K most likely next words and redistribute the probability mass among only those K next words.
  - `"nucleus"`: filter to the smallest possible set of words whose cumulative probability exceeds the probability `p` and redistribute the probability mass among this set of words.
- `temperature`: a control over randomness. As this value approaches zero, the model becomes more deterministic. (default: `1.0`)
- `k`: size of the set of words to consider for "top-k" sampling (default: `5`)
- `p`: a control over diversity in nucleus sampling. A value of 0.5 means that half of the options are considered. (default: `0.9`)
- `max_decoding_steps`: number of tokens to generate. (default: `32`)
- `skip_eos`: when `True`, generation does not stop at end of sentence. (default: `True`)

## <img src="https://cloud.lighton.ai/wp-content/uploads/2020/01/LightOnCloud.png" width=50/> More on LightOn

LightOn is a company that produces hardware for machine learning.
To lease a LightOn Appliance, please visit: https://lighton.ai/lighton-appliance/

To request access to LightOn Cloud and try our photonic co-processor, please visit: https://cloud.lighton.ai/
For researchers, we also have a LightOn Cloud for Research program, please visit https://cloud.lighton.ai/lighton-research/ for more information.

## Citation

We will soon have a preprint on arXiv, stay tuned ;)


