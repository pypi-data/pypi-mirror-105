# Autogenerated file.
import glob
import os

import numpy as np
import onnx
import onnx.numpy_helper
import torch
import torch.nn as nn


class Model(nn.Module):

  def __init__(self):
    super(Model, self).__init__()
    self.__variables = {
        os.path.basename(b)[:-4]: torch.from_numpy(np.load(b))
        for b in glob.glob(
            os.path.join(os.path.dirname(__file__), "variables", "*.npy"))
        if os.path.isfile(b)
    }

    self.Conv_0 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 64,
            'padding': [3, 3],
            'kernel_size': (7, 7),
            'stride': [2, 2],
            'in_channels': 3
        })
    self.Conv_0.reset_parameters()
    self.Conv_0.weight.data = self.__variables["conv1_weight"]
    self.BatchNormalization_1 = nn.BatchNorm2d(
        **{
            'num_features': 64,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_1.weight.data = self.__variables["bn1_weight"]
    self.BatchNormalization_1.bias.data = self.__variables["bn1_bias"]
    self.Relu_2 = nn.ReLU()
    self.MaxPool_3 = nn.MaxPool2d(
        **{
            'dilation': 1,
            'kernel_size': [3, 3],
            'ceil_mode': False,
            'stride': [2, 2],
            'return_indices': False
        })
    self.Conv_4 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 64,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [1, 1],
            'in_channels': 64
        })
    self.Conv_4.weight.data = self.__variables["layer1_0_conv1_weight"]
    self.BatchNormalization_5 = nn.BatchNorm2d(
        **{
            'num_features': 64,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_5.weight.data = self.__variables[
        "layer1_0_bn1_weight"]
    self.BatchNormalization_5.bias.data = self.__variables["layer1_0_bn1_bias"]
    self.Relu_6 = nn.ReLU()
    self.Conv_7 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 64,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [1, 1],
            'in_channels': 64
        })
    self.Conv_7.weight.data = self.__variables["layer1_0_conv2_weight"]
    self.BatchNormalization_8 = nn.BatchNorm2d(
        **{
            'num_features': 64,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_8.weight.data = self.__variables[
        "layer1_0_bn2_weight"]
    self.BatchNormalization_8.bias.data = self.__variables["layer1_0_bn2_bias"]
    self.Relu_10 = nn.ReLU()
    self.Conv_11 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 64,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [1, 1],
            'in_channels': 64
        })
    self.Conv_11.weight.data = self.__variables["layer1_1_conv1_weight"]
    self.BatchNormalization_12 = nn.BatchNorm2d(
        **{
            'num_features': 64,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_12.weight.data = self.__variables[
        "layer1_1_bn1_weight"]
    self.BatchNormalization_12.bias.data = self.__variables["layer1_1_bn1_bias"]
    self.Relu_13 = nn.ReLU()
    self.Conv_14 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 64,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [1, 1],
            'in_channels': 64
        })
    self.Conv_14.weight.data = self.__variables["layer1_1_conv2_weight"]
    self.BatchNormalization_15 = nn.BatchNorm2d(
        **{
            'num_features': 64,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_15.weight.data = self.__variables[
        "layer1_1_bn2_weight"]
    self.BatchNormalization_15.bias.data = self.__variables["layer1_1_bn2_bias"]
    self.Relu_17 = nn.ReLU()
    self.Conv_18 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 128,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [2, 2],
            'in_channels': 64
        })
    self.Conv_18.weight.data = self.__variables["layer2_0_conv1_weight"]
    self.BatchNormalization_19 = nn.BatchNorm2d(
        **{
            'num_features': 128,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_19.weight.data = self.__variables[
        "layer2_0_bn1_weight"]
    self.BatchNormalization_19.bias.data = self.__variables["layer2_0_bn1_bias"]
    self.Relu_20 = nn.ReLU()
    self.Conv_21 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 128,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [1, 1],
            'in_channels': 128
        })
    self.Conv_21.weight.data = self.__variables["layer2_0_conv2_weight"]
    self.BatchNormalization_22 = nn.BatchNorm2d(
        **{
            'num_features': 128,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_22.weight.data = self.__variables[
        "layer2_0_bn2_weight"]
    self.BatchNormalization_22.bias.data = self.__variables["layer2_0_bn2_bias"]
    self.Conv_23 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 128,
            'padding': [0, 0],
            'kernel_size': (1, 1),
            'stride': [2, 2],
            'in_channels': 64
        })
    self.Conv_23.weight.data = self.__variables["layer2_0_downsample_0_weight"]
    self.BatchNormalization_24 = nn.BatchNorm2d(
        **{
            'num_features': 128,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_24.weight.data = self.__variables[
        "layer2_0_downsample_1_weight"]
    self.BatchNormalization_24.bias.data = self.__variables[
        "layer2_0_downsample_1_bias"]
    self.Relu_26 = nn.ReLU()
    self.Conv_27 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 128,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [1, 1],
            'in_channels': 128
        })
    self.Conv_27.weight.data = self.__variables["layer2_1_conv1_weight"]
    self.BatchNormalization_28 = nn.BatchNorm2d(
        **{
            'num_features': 128,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_28.weight.data = self.__variables[
        "layer2_1_bn1_weight"]
    self.BatchNormalization_28.bias.data = self.__variables["layer2_1_bn1_bias"]
    self.Relu_29 = nn.ReLU()
    self.Conv_30 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 128,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [1, 1],
            'in_channels': 128
        })
    self.Conv_30.weight.data = self.__variables["layer2_1_conv2_weight"]
    self.BatchNormalization_31 = nn.BatchNorm2d(
        **{
            'num_features': 128,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_31.weight.data = self.__variables[
        "layer2_1_bn2_weight"]
    self.BatchNormalization_31.bias.data = self.__variables["layer2_1_bn2_bias"]
    self.Relu_33 = nn.ReLU()
    self.Conv_34 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 256,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [2, 2],
            'in_channels': 128
        })
    self.Conv_34.weight.data = self.__variables["layer3_0_conv1_weight"]
    self.BatchNormalization_35 = nn.BatchNorm2d(
        **{
            'num_features': 256,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_35.weight.data = self.__variables[
        "layer3_0_bn1_weight"]
    self.BatchNormalization_35.bias.data = self.__variables["layer3_0_bn1_bias"]
    self.Relu_36 = nn.ReLU()
    self.Conv_37 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 256,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [1, 1],
            'in_channels': 256
        })
    self.Conv_37.weight.data = self.__variables["layer3_0_conv2_weight"]
    self.BatchNormalization_38 = nn.BatchNorm2d(
        **{
            'num_features': 256,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_38.weight.data = self.__variables[
        "layer3_0_bn2_weight"]
    self.BatchNormalization_38.bias.data = self.__variables["layer3_0_bn2_bias"]
    self.Conv_39 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 256,
            'padding': [0, 0],
            'kernel_size': (1, 1),
            'stride': [2, 2],
            'in_channels': 128
        })
    self.Conv_39.weight.data = self.__variables["layer3_0_downsample_0_weight"]
    self.BatchNormalization_40 = nn.BatchNorm2d(
        **{
            'num_features': 256,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_40.weight.data = self.__variables[
        "layer3_0_downsample_1_weight"]
    self.BatchNormalization_40.bias.data = self.__variables[
        "layer3_0_downsample_1_bias"]
    self.Relu_42 = nn.ReLU()
    self.Conv_43 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 256,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [1, 1],
            'in_channels': 256
        })
    self.Conv_43.weight.data = self.__variables["layer3_1_conv1_weight"]
    self.BatchNormalization_44 = nn.BatchNorm2d(
        **{
            'num_features': 256,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_44.weight.data = self.__variables[
        "layer3_1_bn1_weight"]
    self.BatchNormalization_44.bias.data = self.__variables["layer3_1_bn1_bias"]
    self.Relu_45 = nn.ReLU()
    self.Conv_46 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 256,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [1, 1],
            'in_channels': 256
        })
    self.Conv_46.weight.data = self.__variables["layer3_1_conv2_weight"]
    self.BatchNormalization_47 = nn.BatchNorm2d(
        **{
            'num_features': 256,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_47.weight.data = self.__variables[
        "layer3_1_bn2_weight"]
    self.BatchNormalization_47.bias.data = self.__variables["layer3_1_bn2_bias"]
    self.Relu_49 = nn.ReLU()
    self.Conv_50 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 512,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [2, 2],
            'in_channels': 256
        })
    self.Conv_50.weight.data = self.__variables["layer4_0_conv1_weight"]
    self.BatchNormalization_51 = nn.BatchNorm2d(
        **{
            'num_features': 512,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_51.weight.data = self.__variables[
        "layer4_0_bn1_weight"]
    self.BatchNormalization_51.bias.data = self.__variables["layer4_0_bn1_bias"]
    self.Relu_52 = nn.ReLU()
    self.Conv_53 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 512,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [1, 1],
            'in_channels': 512
        })
    self.Conv_53.weight.data = self.__variables["layer4_0_conv2_weight"]
    self.BatchNormalization_54 = nn.BatchNorm2d(
        **{
            'num_features': 512,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_54.weight.data = self.__variables[
        "layer4_0_bn2_weight"]
    self.BatchNormalization_54.bias.data = self.__variables["layer4_0_bn2_bias"]
    self.Conv_55 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 512,
            'padding': [0, 0],
            'kernel_size': (1, 1),
            'stride': [2, 2],
            'in_channels': 256
        })
    self.Conv_55.weight.data = self.__variables["layer4_0_downsample_0_weight"]
    self.BatchNormalization_56 = nn.BatchNorm2d(
        **{
            'num_features': 512,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_56.weight.data = self.__variables[
        "layer4_0_downsample_1_weight"]
    self.BatchNormalization_56.bias.data = self.__variables[
        "layer4_0_downsample_1_bias"]
    self.Relu_58 = nn.ReLU()
    self.Conv_59 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 512,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [1, 1],
            'in_channels': 512
        })
    self.Conv_59.weight.data = self.__variables["layer4_1_conv1_weight"]
    self.BatchNormalization_60 = nn.BatchNorm2d(
        **{
            'num_features': 512,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_60.weight.data = self.__variables[
        "layer4_1_bn1_weight"]
    self.BatchNormalization_60.bias.data = self.__variables["layer4_1_bn1_bias"]
    self.Relu_61 = nn.ReLU()
    self.Conv_62 = nn.Conv2d(
        **{
            'groups': 1,
            'dilation': [1, 1],
            'out_channels': 512,
            'padding': [1, 1],
            'kernel_size': (3, 3),
            'stride': [1, 1],
            'in_channels': 512
        })
    self.Conv_62.weight.data = self.__variables["layer4_1_conv2_weight"]
    self.BatchNormalization_63 = nn.BatchNorm2d(
        **{
            'num_features': 512,
            'eps': 9.999999747378752e-06,
            'momentum': 0.8999999761581421
        })
    self.BatchNormalization_63.weight.data = self.__variables[
        "layer4_1_bn2_weight"]
    self.BatchNormalization_63.bias.data = self.__variables["layer4_1_bn2_bias"]
    self.Relu_65 = nn.ReLU()
    self.GlobalAveragePool_66 = nn.functional.avg_pool2d
    self.Flatten_67 = nn.Flatten(**{'start_dim': 1})

  def forward(self, x):
    self.input_1 = x
    self.__t_123 = self.Conv_0(self.input_1)
    self.__t_124 = self.BatchNormalization_1(self.__t_123)
    self.__t_129 = self.Relu_2(self.__t_124)
    self.__t_129 = torch.nn.functional.pad(self.__t_129, [2, 2])
    self.__t_130 = self.MaxPool_3(self.__t_129)
    self.__t_131 = self.Conv_4(self.__t_130)
    self.__t_132 = self.BatchNormalization_5(self.__t_131)
    self.__t_137 = self.Relu_6(self.__t_132)
    self.__t_138 = self.Conv_7(self.__t_137)
    self.__t_139 = self.BatchNormalization_8(self.__t_138)
    self.__t_144 = self.__t_139 + self.__t_130
    self.__t_145 = self.Relu_10(self.__t_144)
    self.__t_146 = self.Conv_11(self.__t_145)
    self.__t_147 = self.BatchNormalization_12(self.__t_146)
    self.__t_152 = self.Relu_13(self.__t_147)
    self.__t_153 = self.Conv_14(self.__t_152)
    self.__t_154 = self.BatchNormalization_15(self.__t_153)
    self.__t_159 = self.__t_154 + self.__t_145
    self.__t_160 = self.Relu_17(self.__t_159)
    self.__t_161 = self.Conv_18(self.__t_160)
    self.__t_162 = self.BatchNormalization_19(self.__t_161)
    self.__t_167 = self.Relu_20(self.__t_162)
    self.__t_168 = self.Conv_21(self.__t_167)
    self.__t_169 = self.BatchNormalization_22(self.__t_168)
    self.__t_174 = self.Conv_23(self.__t_160)
    self.__t_175 = self.BatchNormalization_24(self.__t_174)
    self.__t_180 = self.__t_169 + self.__t_175
    self.__t_181 = self.Relu_26(self.__t_180)
    self.__t_182 = self.Conv_27(self.__t_181)
    self.__t_183 = self.BatchNormalization_28(self.__t_182)
    self.__t_188 = self.Relu_29(self.__t_183)
    self.__t_189 = self.Conv_30(self.__t_188)
    self.__t_190 = self.BatchNormalization_31(self.__t_189)
    self.__t_195 = self.__t_190 + self.__t_181
    self.__t_196 = self.Relu_33(self.__t_195)
    self.__t_197 = self.Conv_34(self.__t_196)
    self.__t_198 = self.BatchNormalization_35(self.__t_197)
    self.__t_203 = self.Relu_36(self.__t_198)
    self.__t_204 = self.Conv_37(self.__t_203)
    self.__t_205 = self.BatchNormalization_38(self.__t_204)
    self.__t_210 = self.Conv_39(self.__t_196)
    self.__t_211 = self.BatchNormalization_40(self.__t_210)
    self.__t_216 = self.__t_205 + self.__t_211
    self.__t_217 = self.Relu_42(self.__t_216)
    self.__t_218 = self.Conv_43(self.__t_217)
    self.__t_219 = self.BatchNormalization_44(self.__t_218)
    self.__t_224 = self.Relu_45(self.__t_219)
    self.__t_225 = self.Conv_46(self.__t_224)
    self.__t_226 = self.BatchNormalization_47(self.__t_225)
    self.__t_231 = self.__t_226 + self.__t_217
    self.__t_232 = self.Relu_49(self.__t_231)
    self.__t_233 = self.Conv_50(self.__t_232)
    self.__t_234 = self.BatchNormalization_51(self.__t_233)
    self.__t_239 = self.Relu_52(self.__t_234)
    self.__t_240 = self.Conv_53(self.__t_239)
    self.__t_241 = self.BatchNormalization_54(self.__t_240)
    self.__t_246 = self.Conv_55(self.__t_232)
    self.__t_247 = self.BatchNormalization_56(self.__t_246)
    self.__t_252 = self.__t_241 + self.__t_247
    self.__t_253 = self.Relu_58(self.__t_252)
    self.__t_254 = self.Conv_59(self.__t_253)
    self.__t_255 = self.BatchNormalization_60(self.__t_254)
    self.__t_260 = self.Relu_61(self.__t_255)
    self.__t_261 = self.Conv_62(self.__t_260)
    self.__t_262 = self.BatchNormalization_63(self.__t_261)
    self.__t_267 = self.__t_262 + self.__t_253
    self.__t_268 = self.Relu_65(self.__t_267)
    self.__t_269 = self.GlobalAveragePool_66(
        self.__t_268, **{'kernel_size': self.__t_268.shape[2:]})
    self.__t_270 = self.Flatten_67(self.__t_269)
    self.__t_271 = torch.matmul(
        self.__t_270, torch.transpose(self.__variables["fc_weight"], 0, 1))
    self.__t_271 *= 1.0

    return self.__t_271


model = Model()
model.eval()
inp = np.random.randn(1, 3, 224, 224).astype(np.float32)
with torch.no_grad():
  print(model(torch.from_numpy(inp)))

import onnxruntime
from onnx.helper import make_empty_tensor_value_info
onnx_model = onnx.load("tests/ort_train_resnet18.onnx")
# for o in (["__t_123"]):
#   onnx_model.graph.output.append(make_empty_tensor_value_info(o))
sess_options = onnxruntime.SessionOptions()
session = onnxruntime.InferenceSession(onnx_model.SerializeToString(),
                                       sess_options)
inputs = {"input_1": inp}
ort_outputs = session.run(None, inputs)
print(ort_outputs)



model = Model()
model.train()
# model.eval()
b = 10
inp = [
  ("x_5:0", np.random.randint(0, 188, (b, 1)).astype(np.int32)),
  ("x_4:0", np.random.randint(0, 20, (b, 1)).astype(np.int32)),
  ("x_3:0", np.random.randint(0, 7, (b, 1)).astype(np.int32)),
  ("x_2:0", np.random.randint(0, 2, (b, 1)).astype(np.int32)),
  ("x_1:0", np.random.randint(0, 193, (b, 1)).astype(np.int32)),
  ("x:0", np.random.randint(0, 187, (b, 1)).astype(np.int32)),
]
with torch.no_grad():
  torch_outputs = model(*[torch.from_numpy(v) for _, v in inp])
  print(torch_outputs)

import onnx
import onnxruntime
from onnx.helper import make_empty_tensor_value_info
onnx_model = onnx.load("/Users/wenhao/Projects/DeepCTR/model.onnx")
# for o in (["model/prediction_layer/BiasAdd:0"]):
#   onnx_model.graph.output.append(make_empty_tensor_value_info(o))
sess_options = onnxruntime.SessionOptions()
session = onnxruntime.InferenceSession(onnx_model.SerializeToString(),
                                       sess_options)
inputs = {_: v for _, v in inp}
ort_outputs = session.run(None, inputs)
print(ort_outputs)
